{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denizozturk21/1300ABTesting/blob/master/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoNAv2d0im5d",
        "outputId": "5ee2bfe5-57fb-4bfb-ee65-b249650391b2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI-iWWIrWKpo"
      },
      "source": [
        "def preprocess(filepath):# load data\n",
        "\n",
        "  print(filepath)\n",
        "\n",
        "  tokenized_final = []\n",
        "  file = open(filepath, 'rt')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "\n",
        "  # remove punctuation from each word but periods\n",
        "  text = text.translate(str.maketrans('', '', '!\"#$%&()*+,-/:;<=>?@[\\]^_`{|}~'))\n",
        "\n",
        "\n",
        "  sentences = sent_tokenize(text)\n",
        "\n",
        "  tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(sentences, target_vocab_size=2**13)\n",
        "\n",
        "  #remove newlines and check for sentence length\n",
        "  for x in range(len(sentences)):\n",
        "    sentence = sentences[x]\n",
        "    sentence.replace('\\n', ' ')\n",
        "    if len(sentence.split()) < 8:\n",
        "      sentences[x] = \"?\"\n",
        "  \n",
        "  print(sentences[:100])\n",
        "  #tokenize\n",
        "  tokenized_sents = [word_tokenize(i) for i in sentences]\n",
        "  flattened_tokeninized_final = [i for j in tokenized_sents for i in j]\n",
        "\n",
        "  #set all to lower case\n",
        "  tokens = [w.lower() for w in flattened_tokeninized_final]\n",
        "  print(tokens[:100])\n",
        "  \n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  words = [word for word in tokens if word.isdigit() == False]\n",
        "\n",
        "  # print(words[:100])\n",
        "\n",
        "  # filter out stop words if needed\n",
        "  # stop_words = set(stopwords.words('english'))\n",
        "  # words = [w for w in words if not w in stop_words]\n",
        "  print(words[:100])\n",
        "  return words, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMsgK47cdgnq",
        "outputId": "b9a75307-4159-4404-faa7-513eef06984a"
      },
      "source": [
        "# quran_filepath = \"/content/English-Maulana-Mohammad-Ali-103.txt\"\n",
        "# quran = preprocess(quran_filepath)\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "bible_filepath = \"/content/bible.txt\"\n",
        "bible, tokenizer = preprocess(bible_filepath)\n",
        "\n",
        "\n",
        "# print(bible[:100])\n",
        "print(len(bible)) #787849 #820257 with periods\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bible.txt\n",
            "['11 In the beginning God created the heaven and the earth.', '12 And the earth was without form and void and darkness was upon\\nthe face of the deep.', 'And the Spirit of God moved upon the face of the\\nwaters.', '13 And God said Let there be light and there was light.', '14 And God saw the light that it was good and God divided the light\\nfrom the darkness.', '15 And God called the light Day and the darkness he called Night.', 'And the evening and the morning were the first day.', '16 And God said Let there be a firmament in the midst of the waters\\nand let it divide the waters from the waters.', '17 And God made the firmament and divided the waters which were\\nunder the firmament from the waters which were above the firmament\\nand it was so.', '?', 'And the evening and the\\nmorning were the second day.', '19 And God said Let the waters under the heaven be gathered together\\nunto one place and let the dry land appear and it was so.', '110 And God called the dry land Earth and the gathering together of\\nthe waters called he Seas and God saw that it was good.', '111 And God said Let the earth bring forth grass the herb yielding\\nseed and the fruit tree yielding fruit after his kind whose seed is\\nin itself upon the earth and it was so.', '112 And the earth brought forth grass and herb yielding seed after\\nhis kind and the tree yielding fruit whose seed was in itself after\\nhis kind and God saw that it was good.', '113 And the evening and the morning were the third day.', '114 And God said Let there be lights in the firmament of the heaven\\nto divide the day from the night and let them be for signs and for\\nseasons and for days and years 115 And let them be for lights in\\nthe firmament of the heaven to give light upon the earth and it was\\nso.', '116 And God made two great lights the greater light to rule the day\\nand the lesser light to rule the night he made the stars also.', '117 And God set them in the firmament of the heaven to give light\\nupon the earth 118 And to rule over the day and over the night and\\nto divide the light from the darkness and God saw that it was good.', '119 And the evening and the morning were the fourth day.', '120 And God said Let the waters bring forth abundantly the moving\\ncreature that hath life and fowl that may fly above the earth in the\\nopen firmament of heaven.', '121 And God created great whales and every living creature that\\nmoveth which the waters brought forth abundantly after their kind\\nand every winged fowl after his kind and God saw that it was good.', '122 And God blessed them saying Be fruitful and multiply and fill\\nthe waters in the seas and let fowl multiply in the earth.', '123 And the evening and the morning were the fifth day.', '124 And God said Let the earth bring forth the living creature after\\nhis kind cattle and creeping thing and beast of the earth after his\\nkind and it was so.', '125 And God made the beast of the earth after his kind and cattle\\nafter their kind and every thing that creepeth upon the earth after\\nhis kind and God saw that it was good.', '126 And God said Let us make man in our image after our likeness\\nand let them have dominion over the fish of the sea and over the fowl\\nof the air and over the cattle and over all the earth and over\\nevery creeping thing that creepeth upon the earth.', '127 So God created man in his own image in the image of God created\\nhe him male and female created he them.', '128 And God blessed them and God said unto them Be fruitful and\\nmultiply and replenish the earth and subdue it and have dominion\\nover the fish of the sea and over the fowl of the air and over every\\nliving thing that moveth upon the earth.', '129 And God said Behold I have given you every herb bearing seed\\nwhich is upon the face of all the earth and every tree in the which\\nis the fruit of a tree yielding seed to you it shall be for meat.', '130 And to every beast of the earth and to every fowl of the air\\nand to every thing that creepeth upon the earth wherein there is\\nlife I have given every green herb for meat and it was so.', '131 And God saw every thing that he had made and behold it was\\nvery good.', 'And the evening and the morning were the sixth day.', '21 Thus the heavens and the earth were finished and all the host of\\nthem.', '22 And on the seventh day God ended his work which he had made and\\nhe rested on the seventh day from all his work which he had made.', '23 And God blessed the seventh day and sanctified it because that\\nin it he had rested from all his work which God created and made.', '24 These are the generations of the heavens and of the earth when\\nthey were created in the day that the LORD God made the earth and the\\nheavens 25 And every plant of the field before it was in the earth\\nand every herb of the field before it grew for the LORD God had not\\ncaused it to rain upon the earth and there was not a man to till the\\nground.', '26 But there went up a mist from the earth and watered the whole\\nface of the ground.', '27 And the LORD God formed man of the dust of the ground and\\nbreathed into his nostrils the breath of life and man became a living\\nsoul.', '28 And the LORD God planted a garden eastward in Eden and there he\\nput the man whom he had formed.', '29 And out of the ground made the LORD God to grow every tree that is\\npleasant to the sight and good for food the tree of life also in the\\nmidst of the garden and the tree of knowledge of good and evil.', '210 And a river went out of Eden to water the garden and from thence\\nit was parted and became into four heads.', '211 The name of the first is Pison that is it which compasseth the\\nwhole land of Havilah where there is gold 212 And the gold of that\\nland is good there is bdellium and the onyx stone.', '213 And the name of the second river is Gihon the same is it that\\ncompasseth the whole land of Ethiopia.', '214 And the name of the third river is Hiddekel that is it which\\ngoeth toward the east of Assyria.', '?', '215 And the LORD God took the man and put him into the garden of\\nEden to dress it and to keep it.', '216 And the LORD God commanded the man saying Of every tree of the\\ngarden thou mayest freely eat 217 But of the tree of the knowledge\\nof good and evil thou shalt not eat of it for in the day that thou\\neatest thereof thou shalt surely die.', '218 And the LORD God said It is not good that the man should be\\nalone I will make him an help meet for him.', '219 And out of the ground the LORD God formed every beast of the\\nfield and every fowl of the air and brought them unto Adam to see\\nwhat he would call them and whatsoever Adam called every living\\ncreature that was the name thereof.', '220 And Adam gave names to all cattle and to the fowl of the air\\nand to every beast of the field but for Adam there was not found an\\nhelp meet for him.', '221 And the LORD God caused a deep sleep to fall upon Adam and he\\nslept and he took one of his ribs and closed up the flesh instead\\nthereof 222 And the rib which the LORD God had taken from man made\\nhe a woman and brought her unto the man.', '223 And Adam said This is now bone of my bones and flesh of my\\nflesh she shall be called Woman because she was taken out of Man.', '224 Therefore shall a man leave his father and his mother and shall\\ncleave unto his wife and they shall be one flesh.', '225 And they were both naked the man and his wife and were not\\nashamed.', '31 Now the serpent was more subtil than any beast of the field which\\nthe LORD God had made.', 'And he said unto the woman Yea hath God said\\nYe shall not eat of every tree of the garden  32 And the woman said\\nunto the serpent We may eat of the fruit of the trees of the garden\\n33 But of the fruit of the tree which is in the midst of the garden\\nGod hath said Ye shall not eat of it neither shall ye touch it lest\\nye die.', '34 And the serpent said unto the woman Ye shall not surely die 35\\nFor God doth know that in the day ye eat thereof then your eyes shall\\nbe opened and ye shall be as gods knowing good and evil.', '36 And when the woman saw that the tree was good for food and that\\nit was pleasant to the eyes and a tree to be desired to make one\\nwise she took of the fruit thereof and did eat and gave also unto\\nher husband with her and he did eat.', '37 And the eyes of them both were opened and they knew that they\\nwere naked and they sewed fig leaves together and made themselves\\naprons.', '38 And they heard the voice of the LORD God walking in the garden in\\nthe cool of the day and Adam and his wife hid themselves from the\\npresence of the LORD God amongst the trees of the garden.', '39 And the LORD God called unto Adam and said unto him Where art\\nthou  310 And he said I heard thy voice in the garden and I was\\nafraid because I was naked and I hid myself.', '311 And he said Who told thee that thou wast naked Hast thou eaten\\nof the tree whereof I commanded thee that thou shouldest not eat\\n312 And the man said The woman whom thou gavest to be with me she\\ngave me of the tree and I did eat.', '313 And the LORD God said unto the woman What is this that thou hast\\ndone And the woman said The serpent beguiled me and I did eat.', '314 And the LORD God said unto the serpent Because thou hast done\\nthis thou art cursed above all cattle and above every beast of the\\nfield upon thy belly shalt thou go and dust shalt thou eat all the\\ndays of thy life 315 And I will put enmity between thee and the\\nwoman and between thy seed and her seed it shall bruise thy head\\nand thou shalt bruise his heel.', '316 Unto the woman he said I will greatly multiply thy sorrow and\\nthy conception in sorrow thou shalt bring forth children and thy\\ndesire shall be to thy husband and he shall rule over thee.', '317 And unto Adam he said Because thou hast hearkened unto the voice\\nof thy wife and hast eaten of the tree of which I commanded thee\\nsaying Thou shalt not eat of it cursed is the ground for thy sake\\nin sorrow shalt thou eat of it all the days of thy life 318 Thorns\\nalso and thistles shall it bring forth to thee and thou shalt eat the\\nherb of the field 319 In the sweat of thy face shalt thou eat bread\\ntill thou return unto the ground for out of it wast thou taken for\\ndust thou art and unto dust shalt thou return.', '320 And Adam called his wife’s name Eve because she was the mother\\nof all living.', '321 Unto Adam also and to his wife did the LORD God make coats of\\nskins and clothed them.', '322 And the LORD God said Behold the man is become as one of us to\\nknow good and evil and now lest he put forth his hand and take also\\nof the tree of life and eat and live for ever 323 Therefore the\\nLORD God sent him forth from the garden of Eden to till the ground\\nfrom whence he was taken.', '324 So he drove out the man and he placed at the east of the garden\\nof Eden Cherubims and a flaming sword which turned every way to keep\\nthe way of the tree of life.', '41 And Adam knew Eve his wife and she conceived and bare Cain and\\nsaid I have gotten a man from the LORD.', '42 And she again bare his brother Abel.', 'And Abel was a keeper of\\nsheep but Cain was a tiller of the ground.', '43 And in process of time it came to pass that Cain brought of the\\nfruit of the ground an offering unto the LORD.', '44 And Abel he also brought of the firstlings of his flock and of\\nthe fat thereof.', 'And the LORD had respect unto Abel and to his\\noffering 45 But unto Cain and to his offering he had not respect.', 'And Cain was very wroth and his countenance fell.', '46 And the LORD said unto Cain Why art thou wroth and why is thy\\ncountenance fallen  47 If thou doest well shalt thou not be\\naccepted and if thou doest not well sin lieth at the door.', 'And unto\\nthee shall be his desire and thou shalt rule over him.', '48 And Cain talked with Abel his brother and it came to pass when\\nthey were in the field that Cain rose up against Abel his brother\\nand slew him.', '49 And the LORD said unto Cain Where is Abel thy brother And he\\nsaid I know not Am I my brother’s keeper  410 And he said What\\nhast thou done the voice of thy brother’s blood crieth unto me from\\nthe ground.', '411 And now art thou cursed from the earth which hath opened her\\nmouth to receive thy brother’s blood from thy hand 412 When thou\\ntillest the ground it shall not henceforth yield unto thee her\\nstrength a fugitive and a vagabond shalt thou be in the earth.', '413 And Cain said unto the LORD My punishment is greater than I can\\nbear.', '414 Behold thou hast driven me out this day from the face of the\\nearth and from thy face shall I be hid and I shall be a fugitive and\\na vagabond in the earth and it shall come to pass that every one\\nthat findeth me shall slay me.', '415 And the LORD said unto him Therefore whosoever slayeth Cain\\nvengeance shall be taken on him sevenfold.', 'And the LORD set a mark\\nupon Cain lest any finding him should kill him.', '416 And Cain went out from the presence of the LORD and dwelt in the\\nland of Nod on the east of Eden.', '417 And Cain knew his wife and she conceived and bare Enoch and he\\nbuilded a city and called the name of the city after the name of his\\nson Enoch.', '418 And unto Enoch was born Irad and Irad begat Mehujael and\\nMehujael begat Methusael and Methusael begat Lamech.', '419 And Lamech took unto him two wives the name of the one was Adah\\nand the name of the other Zillah.', '420 And Adah bare Jabal he was the father of such as dwell in tents\\nand of such as have cattle.', '421 And his brother’s name was Jubal he was the father of all such\\nas handle the harp and organ.', '422 And Zillah she also bare Tubalcain an instructer of every\\nartificer in brass and iron and the sister of Tubalcain was Naamah.', '423 And Lamech said unto his wives Adah and Zillah Hear my voice\\nye wives of Lamech hearken unto my speech for I have slain a man to\\nmy wounding and a young man to my hurt.', '424 If Cain shall be avenged sevenfold truly Lamech seventy and\\nsevenfold.', '425 And Adam knew his wife again and she bare a son and called his\\nname Seth For God said she hath appointed me another seed instead\\nof Abel whom Cain slew.', '426 And to Seth to him also there was born a son and he called his\\nname Enos then began men to call upon the name of the LORD.', '51 This is the book of the generations of Adam.', 'In the day that God\\ncreated man in the likeness of God made he him 52 Male and female\\ncreated he them and blessed them and called their name Adam in the\\nday when they were created.']\n",
            "['11', 'in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', '12', 'and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '.', 'and', 'the', 'spirit', 'of', 'god', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.', '13', 'and', 'god', 'said', 'let', 'there', 'be', 'light', 'and', 'there', 'was', 'light', '.', '14', 'and', 'god', 'saw', 'the', 'light', 'that', 'it', 'was', 'good', 'and', 'god', 'divided', 'the', 'light', 'from', 'the', 'darkness', '.', '15', 'and', 'god', 'called', 'the', 'light', 'day', 'and', 'the', 'darkness', 'he', 'called', 'night', '.', 'and', 'the', 'evening', 'and', 'the', 'morning', 'were', 'the', 'first', 'day']\n",
            "['in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', 'and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', '.', 'and', 'the', 'spirit', 'of', 'god', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', '.', 'and', 'god', 'said', 'let', 'there', 'be', 'light', 'and', 'there', 'was', 'light', '.', 'and', 'god', 'saw', 'the', 'light', 'that', 'it', 'was', 'good', 'and', 'god', 'divided', 'the', 'light', 'from', 'the', 'darkness', '.', 'and', 'god', 'called', 'the', 'light', 'day', 'and', 'the', 'darkness', 'he', 'called', 'night', '.', 'and', 'the', 'evening', 'and', 'the', 'morning', 'were', 'the', 'first', 'day', '.', 'and', 'god', 'said', 'let']\n",
            "820257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvLtZcJ9UQCC",
        "outputId": "c491b472-5f42-498d-c874-d55466ce32f6"
      },
      "source": [
        "print(len(set(bible))) # 12901"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfYQZaaSHfAp"
      },
      "source": [
        "#print(len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoKKtZx1m4Qq"
      },
      "source": [
        "I wrote a function to get a dictionary of unqiue terms+key values as well as turning everything into 1-hot vectors. Might be kinda slow, since it runs for loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b8Fxux_jXmZ"
      },
      "source": [
        "# :param words: a list of words (strings)\n",
        "# :return: a dictionary containing all unique words in the string,\n",
        "# with a corresponding key value, and a 2-D tensor where each row\n",
        "# is a 1-hot vector.\n",
        "def dict_and_one_hot(words):\n",
        "\t# create a set from the words, to eliminate duplicates\n",
        "\twordsList = set(words)\n",
        "\n",
        "\t# initialize our dictionary as well as our key value placeholder\n",
        "\tdict = {}\n",
        "\ti = 0\n",
        "\n",
        "\t# loop through all words in our set, and add them to our dict\n",
        "\t# and set its value to the key value, then increment our key\n",
        "\t# value placeholder by 1.\n",
        "\tfor word in wordsList:\n",
        "\t\tdict[word] = i\n",
        "\t\ti += 1\n",
        "\n",
        "\t# initialize a list to hold our indices as well as how many\n",
        "\t# different words there are.\n",
        "\tindices = []\n",
        "\tdepth = len(dict)\n",
        "\n",
        "\t# loop through all words, and store their key value as a list.\n",
        "\tfor word in words:\n",
        "\t\tindices.append(dict[word])\n",
        "\n",
        "\t# return the dict, as well as applying tf.one_hot to our list\n",
        "\t# of indices.\n",
        "\treturn dict, indices #tf.one_hot(indices, depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH_KESWzjkJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fd5551-3161-45a0-a18f-fbe11c8ec81e"
      },
      "source": [
        "# function demonstration\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# d, w = dict_and_one_hot(quran)\n",
        "d2, w2 = dict_and_one_hot(bible)\n",
        "print(\"dictionary: \", len(d2))\n",
        "print(\"words as 1-hot vectors: \", w2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qncq6GhmjGA"
      },
      "source": [
        "Below is an example of the lemmatization and stemming functions from the nltk library. However, we may not want to lemmatize or stem the words, because we may want to preserve part of speech instead of just the essence of the word. On the other hand, keeping part of speech in may be a dead giveaway, lol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLd9bWS9j5Cc"
      },
      "source": [
        "# we can also try the lemmatization function\n",
        "#lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "#print('dogs lemmatized is: ', lemmatizer.lemmatize('dogs'))\n",
        "\n",
        "#print()\n",
        "\n",
        "# but it's not great, since it treats everything as a noun. As you can see, it\n",
        "# does nothing to the test sentence\n",
        "#for word in tokenized_test:\n",
        "#  print(word, \" lemmatized is: \", lemmatizer.lemmatize(word))\n",
        "\n",
        "#print()\n",
        "\n",
        "# however, if we specify part of speech, it gets better\n",
        "#print('created lemmatized with PoS is: ', lemmatizer.lemmatize('created', pos='v'))\n",
        "# but specifying part of speech is a bit much to ask for"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S660jQ6k7b0"
      },
      "source": [
        "# we can also try the stem function, but it also has its fallbacks\n",
        "#ps = nltk.stem.PorterStemmer()\n",
        "\n",
        "#for word in tokenized_test:\n",
        "#  print(word, \" stemmed is: \", ps.stem(word))\n",
        "\n",
        "#print()\n",
        "\n",
        "# looks good, but look what happens with this\n",
        "#print('programmer stemmed is: ', ps.stem('programmer'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUMsohH9oFKj"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"\n",
        "        The Model class predicts the next words in a sequence.\n",
        "\n",
        "        :param vocab_size: The number of unique words in the data\n",
        "        \"\"\"\n",
        "\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # TODO: initialize vocab_size, embedding_size\n",
        "        # I filled in all the inital parameters, not much to explain\n",
        "        self.vocab_size = vocab_size\n",
        "        self.window_size = 20 # DO NOT CHANGE!\n",
        "        self.embedding_size = 50\n",
        "        self.batch_size = 100\n",
        "        self.hidden_layer_size = 100\n",
        "        self.rnn_size = 200\n",
        "        self.learning_rate = 0.01\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "        # TODO: initialize embeddings and forward pass weights (weights, biases)\n",
        "        # Note: You can now use tf.keras.layers!\n",
        "        # - use tf.keras.layers.Dense for feed forward layers: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
        "        # - and use tf.keras.layers.GRU or tf.keras.layers.LSTM for your RNN\n",
        "        # Initialize an embedding matrix of size vocab_size x embedding_size\n",
        "        self.embedding_matrix = tf.Variable(tf.random.truncated_normal([self.vocab_size, self.embedding_size],stddev=0.1))\n",
        "\n",
        "        # Initialize an LSTM layer\n",
        "        self.LSTM = tf.keras.layers.LSTM(self.rnn_size, return_sequences=True, return_state=True)\n",
        "\n",
        "        # Initialize 3 Dense layers, the last one with output dimension of vocab_size\n",
        "        self.W1 = tf.keras.layers.Dense(self.hidden_layer_size)\n",
        "\n",
        "        self.W2 = tf.keras.layers.Dense(self.hidden_layer_size)\n",
        "\n",
        "        self.W3 = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "\n",
        "    # made initial_state default to None, b/c that's what it defaults to for LSTM\n",
        "    def call(self, inputs, initial_state=None):\n",
        "        \"\"\"\n",
        "        - You must use an embedding layer as the first layer of your network (i.e. tf.nn.embedding_lookup)\n",
        "        - You must use an LSTM or GRU as the next layer.\n",
        "\n",
        "        :param inputs: word ids of shape (batch_size, window_size)\n",
        "        :param initial_state: 2-d array of shape (batch_size, rnn_size) as a tensor\n",
        "        :return: the batch element probabilities as a tensor, a final_state (Note 1: If you use an LSTM, the final_state will be the last two RNN outputs, \n",
        "        Note 2: We only need to use the initial state during generation)\n",
        "        using LSTM and only the probabilites as a tensor and a final_state as a tensor when using GRU \n",
        "        \"\"\"\n",
        "        \n",
        "        #TODO: Fill in\n",
        "        # Lookup the embeddings\n",
        "        E = tf.nn.embedding_lookup(self.embedding_matrix, inputs)\n",
        "\n",
        "        # apply the LSTM layer, and collect the logits as well as the state outputs\n",
        "        l1, final_state1, final_state2 = self.LSTM(E, initial_state=initial_state)\n",
        "\n",
        "        # apply the 3 dense layers as well as ReLU and Softmax\n",
        "        l2 = self.W1(l1)\n",
        "        l2 = tf.nn.relu(l2)\n",
        "\n",
        "        l3 = self.W2(l2)\n",
        "        l3 = tf.nn.relu(l3)\n",
        "\n",
        "        l4 = self.W3(l3)\n",
        "        l4 = tf.nn.softmax(l4)\n",
        "\n",
        "        # return the final logits, as well as the final state\n",
        "        return l4, (final_state1, final_state2)\n",
        "\n",
        "    def loss(self, probs, labels):\n",
        "        \"\"\"\n",
        "        Calculates average cross entropy sequence to sequence loss of the prediction\n",
        "        \n",
        "        NOTE: You have to use np.reduce_mean and not np.reduce_sum when calculating your loss\n",
        "\n",
        "        :param logits: a matrix of shape (batch_size, window_size, vocab_size) as a tensor\n",
        "        :param labels: matrix of shape (batch_size, window_size) containing the labels\n",
        "        :return: the loss of the model as a tensor of size 1\n",
        "        \"\"\"\n",
        "\n",
        "        #TODO: Fill in\n",
        "        #We recommend using tf.keras.losses.sparse_categorical_crossentropy\n",
        "        #https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy\n",
        "\n",
        "        # apply the loss function\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, probs)\n",
        "\n",
        "        # return the reduced mean of the loss\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "\n",
        "# a function that takes two lists as inputs and shuffles them in the same order.\n",
        "def shuffler(inputs, labels):\n",
        "\t# Get a range of indices, by using tf.range, then shuffle them\n",
        "\tindices = tf.range(start=0, limit=tf.shape(labels)[0], dtype=tf.int32)\n",
        "\tshuffled_indices = tf.random.shuffle(indices)\n",
        "\n",
        "\t# gather the inputs and labels in the order of the shuffled indices\n",
        "\tfinal_inputs = tf.gather(inputs, shuffled_indices)\n",
        "\tfinal_labels = tf.gather(labels, shuffled_indices)\n",
        "\n",
        "\treturn final_inputs, final_labels\n",
        "\n",
        "\n",
        "def train(model, train_inputs, train_labels):\n",
        "    \"\"\"\n",
        "    Runs through one epoch - all training examples.\n",
        "\n",
        "    :param model: the initilized model to use for forward and backward pass\n",
        "    :param train_inputs: train inputs (all inputs for training) of shape (num_inputs,)\n",
        "    :param train_labels: train labels (all labels for training) of shape (num_labels,)\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    #TODO: Fill in\n",
        "\n",
        "\n",
        "    # Since we need a bunch of intervals of size window_size, we determine the largest multiple of window_size\n",
        "    # that is smaller than the size of the training set, and use that as our ending point\n",
        "    ending = (len(train_labels) // model.window_size) * model.window_size\n",
        "\n",
        "    # reshape using the ending we found earlier, so now we can reshape into -1 x window_size\n",
        "    train_inputs = tf.reshape(train_inputs[:ending], [-1, model.window_size])\n",
        "    train_labels = tf.reshape(train_labels[:ending], [-1, model.window_size])\n",
        "\n",
        "    # shuffle training inputs and labels.\n",
        "    inputs, labels = shuffler(train_inputs, train_labels)\n",
        "\n",
        "    # loop through all elements of the epoch\n",
        "    i = 0\n",
        "    while i < (labels.shape[0] - model.batch_size):\n",
        "        #print(\"Training Batch: \", i)\n",
        "        # take out inputs and labels in batches of batches_size\n",
        "        batch_inputs = inputs[i:i + model.batch_size]\n",
        "        batch_labels = labels[i:i + model.batch_size]\n",
        "\n",
        "        # get the tape\n",
        "        with tf.GradientTape() as tape:\n",
        "            # call the model on the inputs\n",
        "            p, state = model.call(batch_inputs)\n",
        "\n",
        "            # get the loss from the predictions\n",
        "            losss = model.loss(p, batch_labels)\n",
        "\n",
        "        # get the gradients and call the optimizer to optimize them\n",
        "        gradients = tape.gradient(losss, model.trainable_variables)\n",
        "        op = model.optimizer\n",
        "        op.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # increment to the next batch\n",
        "        i += model.batch_size\n",
        "\n",
        "\n",
        "def test(model, test_inputs, test_labels):\n",
        "    \"\"\"\n",
        "    Runs through one epoch - all testing examples\n",
        "\n",
        "    :param model: the trained model to use for prediction\n",
        "    :param test_inputs: train inputs (all inputs for testing) of shape (num_inputs,)\n",
        "    :param test_labels: train labels (all labels for testing) of shape (num_labels,)\n",
        "    :returns: perplexity of the test set\n",
        "    \"\"\"\n",
        "    \n",
        "    #TODO: Fill in\n",
        "    #NOTE: Ensure a correct perplexity formula (different from raw loss)\n",
        "\n",
        "    # Since we need a bunch of intervals of size window_size, we determine the largest multiple of window_size\n",
        "    # that is smaller than the size of the training set, and use that as our ending point\n",
        "    ending = (len(test_labels) // model.window_size) * model.window_size\n",
        "\n",
        "    # reshape using the ending we found earlier, so now we can reshape into -1 x window_size\n",
        "    test_inputs = tf.reshape(test_inputs[:ending], [-1, model.window_size])\n",
        "    test_labels = tf.reshape(test_labels[:ending], [-1, model.window_size])\n",
        "\n",
        "    # shuffle testing inputs and labels\n",
        "    inputs, labels = shuffler(test_inputs, test_labels)\n",
        "\n",
        "    i = 0\n",
        "    loss = 0        # we'll need this for perplexity later\n",
        "    num_batches = 0 # we'll need this for perplexity later\n",
        "    while i < (labels.shape[0] - model.batch_size):\n",
        "        #print(\"Testing Batch: \", i)\n",
        "        # take out inputs and labels in batches of batches_size\n",
        "        batch_inputs = inputs[i:i + model.batch_size]\n",
        "        batch_labels = labels[i:i + model.batch_size]\n",
        "\n",
        "        p, state = model.call(batch_inputs)\n",
        "\n",
        "        # get the loss from the predictions\n",
        "        loss = loss + model.loss(p, batch_labels)\n",
        "\n",
        "        # increment to the next batch\n",
        "        i += model.batch_size\n",
        "        num_batches += 1 # count the number of batches\n",
        "\n",
        "    # here, we average the average losses to get the total loss for perplexity\n",
        "    perplexity = loss / num_batches\n",
        "\n",
        "    # perform the last operation to calculate perplexity (exponential operation) and return the result.\n",
        "    return np.exp(perplexity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXbefJgkJJoN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "4a8f291e-fad3-400c-ea2c-5aa5eff6d57c"
      },
      "source": [
        "\n",
        "model = Model(len(d))\n",
        "\n",
        "test_index = np.floor(0.9 * len(quran))\n",
        "test_index = int(test_index)\n",
        "\n",
        "#print(len(w))\n",
        "\n",
        "quran_train = tf.cast(w[:test_index], dtype=tf.int32)\n",
        "quran_test = tf.cast(w[test_index:], dtype=tf.int32)\n",
        "\n",
        "for _ in range(5):\n",
        "  train(model, quran_train[:-1], quran_train[1:])\n",
        "\n",
        "  p = test(model, quran_test[:-1], quran_test[1:])\n",
        "\n",
        "  print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-9e4662347540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquran\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'quran' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFmURYir4ZmO",
        "outputId": "083711dc-cf93-45b9-f8f5-3d9b95ab9a6f"
      },
      "source": [
        "#generation stuff\n",
        "words = np.array([[np.random.randint(len(d))]])\n",
        "for i in range(0,15):\n",
        "  temp, _ = model.call(words)\n",
        "  temp = temp[0]\n",
        "  temp = np.cumsum(temp[-1])\n",
        "  r = np.random.rand()\n",
        "  next = np.where(temp > r)\n",
        "  words = np.array([np.append(words, next[0][0])])\n",
        "\n",
        "inv_map = {v: k for k, v in d.items()}\n",
        "\n",
        "for word in words[0]:\n",
        "  print(inv_map[word])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "forbade\n",
            "war\n",
            "in\n",
            "all\n",
            "things\n",
            "has\n",
            "indeed\n",
            "among\n",
            "me\n",
            "say\n",
            "why\n",
            "serve\n",
            "be\n",
            "blown\n",
            "if\n",
            "you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P9RPmtKG3q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db00cfc-381c-42db-9ccc-22955216670e"
      },
      "source": [
        "model2 = Model(len(d2))\n",
        "for e in range(4):\n",
        "\n",
        "  test_index = np.floor(0.9 * len(w2))\n",
        "  test_index = int(test_index)\n",
        "\n",
        "  #print(len(w2))\n",
        "\n",
        "  bible_train = tf.cast(w2[:test_index], dtype=tf.int32)\n",
        "  bible_test = tf.cast(w2[test_index:], dtype=tf.int32)\n",
        "\n",
        "  train(model2, bible_train[:-1], bible_train[1:])\n",
        "\n",
        "  p = test(model2, bible_test[:-1], bible_test[1:])\n",
        "\n",
        "  print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "315.21527\n",
            "270.6853\n",
            "255.89224\n",
            "257.45886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqbGfxsG52XU"
      },
      "source": [
        "# Generates a sentence from by randomly selecting the next word from the \n",
        "# probability distribution described by the logits.\n",
        "# :param model: a neural net model\n",
        "# :param seed_word: an array of indices of words\n",
        "# :param num_words: how many words we want to generate\n",
        "# :param dictionary: the dictionary containing the vocabulary and its keys and \n",
        "# values\n",
        "# :return: a string representing a sentence.\n",
        "def generate_rand(model, seed_word, num_words, dictionary):\n",
        "  # intialize our sentence to be just the words given\n",
        "  sentence = seed_word\n",
        "  # loop through the number of words we want.\n",
        "  for i in range(0, num_words):\n",
        "    # call the model to get the words\n",
        "    temp, _ = model.call(sentence)\n",
        "    # get the last prediction\n",
        "    temp = temp[0][-1]\n",
        "    # split the probabilities into bins, choose a random number.\n",
        "    temp = np.cumsum(temp)\n",
        "    r = np.random.rand()\n",
        "    # find the bin the random number falls into, i.e which index in the cumsum\n",
        "    # is the first to be larger than our random number.\n",
        "    next = np.where(temp > r)[0][0]\n",
        "    # append our predicted word index to the sentence.\n",
        "    sentence = np.array([np.append(sentence, next)])\n",
        "\n",
        "  # initialize the text we will return as well as create a dictionary that will\n",
        "  # transform key values into strings.\n",
        "  text = []\n",
        "  inv_map = {v: k for k, v in dictionary.items()}\n",
        "\n",
        "  # loop through all words in the sentence and append the string corresponding\n",
        "  # to the index to our list of strings\n",
        "  for word in sentence[0]:\n",
        "    text.append(inv_map[word])\n",
        "\n",
        "  # return our string.\n",
        "  return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjgFnUaB0G8O"
      },
      "source": [
        "# Generates a sentence from by randomly selecting the next word from the \n",
        "# group of top n choices from the probability distribution described by the\n",
        "# logits.\n",
        "# :param model: a neural net model\n",
        "# :param seed_word: an array of indices of words\n",
        "# :param num_words: how many words we want to generate\n",
        "# :param dictionary: the dictionary containing the vocabulary and its keys and \n",
        "# values\n",
        "# :return: a string representing a sentence.\n",
        "def generate_top(model, seed_word, num_words, dictionary, top):\n",
        "  # intialize our sentence to be just the words given\n",
        "  sentence = seed_word\n",
        "  # loop through the number of words we want.\n",
        "  top = -1 * top\n",
        "  for i in range(0, num_words):\n",
        "    # call the model to get the words\n",
        "    temp, _ = model.call(sentence)\n",
        "    # get the last prediction\n",
        "    temp = temp[0][-1]\n",
        "    temp = np.array(temp)\n",
        "    inds = np.argsort(temp)[top:]\n",
        "    args = temp[inds]\n",
        "    # split the probabilities into bins, choose a random number.\n",
        "    args = np.cumsum(args / np.sum(args))\n",
        "    r = np.random.rand()\n",
        "    # find the bin the random number falls into, i.e which index in the cumsum\n",
        "    # is the first to be larger than our random number.\n",
        "    next = inds[np.where(args > r)[0][0]]\n",
        "    # append our predicted word index to the sentence.\n",
        "    sentence = np.array([np.append(sentence, next)])\n",
        "\n",
        "  # initialize the text we will return as well as create a dictionary that will\n",
        "  # transform key values into strings.\n",
        "  text = []\n",
        "  inv_map = {v: k for k, v in dictionary.items()}\n",
        "\n",
        "  # loop through all words in the sentence and append the string corresponding\n",
        "  # to the index to our list of strings\n",
        "  for word in sentence[0]:\n",
        "    text.append(inv_map[word])\n",
        "\n",
        "  # return our string.\n",
        "  return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVYr44TKbh0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b7af0b-9294-4113-a3a0-4d835a52a13a"
      },
      "source": [
        "#generation stuff\n",
        "words1 = np.array([[np.random.randint(len(d2))]])\n",
        "print(generate_top(model2, words1, 30, d2, 10))\n",
        "words = np.array([[np.random.randint(len(d))]])\n",
        "print(generate_top(model, words, 15, d, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "began to reign over him saying the king of israel had brought out the people in their ears . then the son of nethaniah went up into their place and his\n",
            "arelites confessed blasphemer curdled compound compound composition secondarily sabaoth songs therefore fierce songs storehouse fierce hammothdor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CFAnGzFarKZa",
        "outputId": "f2afc608-90b4-4693-9c63-ef36edcbe142"
      },
      "source": [
        "def text_generator(_model, start, temperature=1.0):\n",
        "    result = start\n",
        "    output = [tokenizer.vocab_size] + tokenizer.encode(start)\n",
        "    output = tf.expand_dims(output, 0)  # Expand by adding batch and time dimensions.\n",
        "    for i in range(50):\n",
        "        _model.reset_states()\n",
        "        prediction = _model(output)\n",
        "        print(prediction)\n",
        "        print(\"hi\")\n",
        "        prediction = tf.squeeze(prediction, 0)\n",
        "        print(\"hi\")\n",
        "\n",
        "        prediction = prediction / temperature\n",
        "        print(\"hi\")\n",
        "        prediction = tf.random.categorical(prediction, num_samples=1)[-1, 0]\n",
        "        prediction = tf.squeeze(prediction).numpy()\n",
        "\n",
        "        if prediction == tokenizer.vocab_size+1 or prediction == 0:\n",
        "            return result.strip()\n",
        "\n",
        "        result += tokenizer.decode([prediction])\n",
        "        prediction = tf.expand_dims([prediction], 0)\n",
        "        output = tf.concat([output, prediction], axis=-1)\n",
        "\n",
        "    return result.strip()\n",
        "print(text_generator(model, 'I '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(1, 2, 12901), dtype=float32, numpy=\n",
            "array([[[7.7452576e-05, 7.7491954e-05, 7.7552948e-05, ...,\n",
            "         7.7479999e-05, 7.7491699e-05, 7.7506324e-05],\n",
            "        [7.7500648e-05, 7.7550692e-05, 7.7502336e-05, ...,\n",
            "         7.7505159e-05, 7.7513549e-05, 7.7504701e-05]]], dtype=float32)>, (<tf.Tensor: shape=(1, 200), dtype=float32, numpy=\n",
            "array([[ 1.01817474e-02, -6.60065189e-03, -1.54333387e-03,\n",
            "        -3.66778113e-04, -1.58001762e-02, -4.41129971e-03,\n",
            "         8.89913063e-04, -2.63793929e-03,  6.08793506e-03,\n",
            "        -1.99704082e-03,  6.05016714e-03, -2.96840817e-03,\n",
            "        -6.50398759e-03,  1.62763335e-02,  9.09570698e-03,\n",
            "        -1.61726214e-02,  3.97202140e-03, -1.72349170e-03,\n",
            "        -5.29166544e-03,  6.51297392e-03, -1.19396653e-02,\n",
            "         2.66681723e-02,  1.40450138e-03,  9.87626519e-03,\n",
            "         4.12335759e-03,  2.50258949e-03, -1.26139317e-02,\n",
            "         5.48283383e-03, -1.19053209e-02,  3.04993382e-03,\n",
            "         1.81498360e-02, -5.41173294e-03,  4.45675664e-03,\n",
            "        -4.45305742e-03,  9.03614610e-03,  1.02031184e-02,\n",
            "         1.08452756e-02, -1.17206629e-02,  6.86378870e-03,\n",
            "         3.89099936e-03,  5.34829171e-03, -6.62337756e-03,\n",
            "         1.03661586e-02, -2.59996485e-03,  8.08000378e-03,\n",
            "        -6.04150491e-03, -4.97522904e-03, -9.07968264e-03,\n",
            "        -1.98212406e-03,  9.35052615e-03, -2.57629855e-03,\n",
            "         7.38599245e-03, -1.85954082e-03, -3.01609421e-03,\n",
            "        -9.71403532e-03,  1.15219783e-02, -1.50192333e-02,\n",
            "        -2.07388494e-03,  1.05647026e-02, -8.15719739e-03,\n",
            "         4.04565083e-03,  4.47186176e-03,  1.51268821e-02,\n",
            "         1.10222085e-03, -1.04014277e-02,  3.05686612e-03,\n",
            "         4.36160073e-04, -5.51932864e-03,  2.94063753e-03,\n",
            "         7.73907825e-03,  5.05564827e-03,  1.03141945e-02,\n",
            "         1.14088794e-02,  1.92588184e-03,  9.15101729e-03,\n",
            "        -2.94983131e-03,  7.13529112e-03, -2.97983852e-03,\n",
            "         7.87233748e-03, -1.11012952e-02,  1.11782243e-02,\n",
            "        -1.66321895e-03,  2.09321105e-03, -2.76709022e-03,\n",
            "        -5.05117700e-03, -7.02135591e-03, -9.16369539e-03,\n",
            "        -7.84478430e-03, -1.36961660e-03, -8.07493459e-04,\n",
            "         3.15727689e-03, -8.13155901e-03, -8.96933489e-03,\n",
            "         7.08108419e-04,  8.77124164e-03, -1.12519655e-02,\n",
            "        -1.68780028e-03, -3.86541546e-03,  4.08561295e-03,\n",
            "        -2.96973041e-03, -4.56062052e-03,  8.86827521e-03,\n",
            "        -2.31450703e-02, -1.07208878e-04,  6.41771965e-03,\n",
            "        -8.59898806e-04,  1.40231961e-04,  3.03971441e-03,\n",
            "         8.21635034e-03,  8.62910133e-03,  5.50557347e-03,\n",
            "         6.26281835e-03,  4.31831274e-03, -5.10546518e-03,\n",
            "         4.29419288e-03,  1.72189958e-02,  5.62409114e-04,\n",
            "         1.55071849e-02, -5.33588743e-03, -5.31595899e-03,\n",
            "        -5.04502561e-03,  2.51638051e-03, -1.20866634e-02,\n",
            "         1.82648853e-03,  2.47242348e-03, -2.40367651e-03,\n",
            "         2.09066202e-03,  2.64601107e-03, -4.22678236e-03,\n",
            "         1.05565339e-02, -7.90889096e-03,  2.86577968e-03,\n",
            "        -1.71239283e-02, -1.03997141e-02, -1.33272605e-02,\n",
            "        -8.84466246e-03,  2.67132814e-03,  1.34453652e-02,\n",
            "         2.89718411e-03, -3.65125341e-03, -4.30315081e-03,\n",
            "        -4.08642646e-03, -1.08634718e-02,  6.02590572e-03,\n",
            "        -1.30592166e-02, -4.48218407e-03, -7.32815405e-03,\n",
            "         1.20211784e-02,  2.99626336e-05,  4.16991953e-03,\n",
            "        -6.68356195e-03, -1.20251346e-02,  1.54382112e-02,\n",
            "        -1.01071997e-02,  1.31989177e-02,  1.09773744e-02,\n",
            "         1.42973280e-02,  1.84938554e-02, -1.11306496e-02,\n",
            "         8.15545768e-03,  7.74168922e-03, -1.65071909e-03,\n",
            "        -1.54455577e-03,  4.23328136e-04,  1.36726163e-02,\n",
            "         8.62278789e-03, -1.31234219e-02,  4.74142889e-03,\n",
            "         2.40431027e-03, -7.42584607e-03, -1.35266213e-02,\n",
            "        -1.07471216e-02,  2.96705659e-03,  2.73521314e-03,\n",
            "        -3.28605087e-03, -8.65755137e-03,  1.07521415e-02,\n",
            "        -6.51050685e-03, -2.50507379e-04,  9.05947667e-03,\n",
            "        -4.44137491e-03,  2.07840987e-02,  4.72901901e-03,\n",
            "        -7.42902164e-04, -6.54743379e-03, -2.97745503e-03,\n",
            "        -1.70086685e-03, -2.26017204e-03,  5.95213193e-03,\n",
            "         1.89133722e-03,  1.11514539e-03, -9.97870229e-03,\n",
            "        -8.93561635e-03, -8.86449218e-03, -7.28762010e-03,\n",
            "        -1.13140596e-02, -2.88574561e-03, -3.04838573e-03,\n",
            "         6.21700147e-03,  8.24795477e-03]], dtype=float32)>, <tf.Tensor: shape=(1, 200), dtype=float32, numpy=\n",
            "array([[ 2.02343687e-02, -1.29636675e-02, -3.12981475e-03,\n",
            "        -7.19573640e-04, -3.19081880e-02, -8.76213238e-03,\n",
            "         1.76191900e-03, -5.27502969e-03,  1.21457521e-02,\n",
            "        -3.89244664e-03,  1.22324731e-02, -5.91178611e-03,\n",
            "        -1.26799578e-02,  3.21228430e-02,  1.86100565e-02,\n",
            "        -3.26629914e-02,  7.84444343e-03, -3.40408226e-03,\n",
            "        -1.06579205e-02,  1.30740125e-02, -2.40876917e-02,\n",
            "         5.28729707e-02,  2.81300303e-03,  1.98494494e-02,\n",
            "         8.18935223e-03,  5.07204700e-03, -2.52690408e-02,\n",
            "         1.06081627e-02, -2.40836833e-02,  6.22508256e-03,\n",
            "         3.64651158e-02, -1.09167406e-02,  8.68222769e-03,\n",
            "        -8.83410219e-03,  1.79163441e-02,  2.07418073e-02,\n",
            "         2.21449379e-02, -2.32599173e-02,  1.35113448e-02,\n",
            "         7.57174101e-03,  1.07631274e-02, -1.28777660e-02,\n",
            "         2.10221503e-02, -5.16203977e-03,  1.64126083e-02,\n",
            "        -1.20502301e-02, -1.01170829e-02, -1.82731673e-02,\n",
            "        -4.01395420e-03,  1.84692554e-02, -5.16938418e-03,\n",
            "         1.47891901e-02, -3.76591180e-03, -5.96911926e-03,\n",
            "        -1.97577383e-02,  2.31036004e-02, -3.00251544e-02,\n",
            "        -4.06743819e-03,  2.09327787e-02, -1.62926484e-02,\n",
            "         8.08022078e-03,  9.16507468e-03,  3.00965253e-02,\n",
            "         2.19787704e-03, -2.05419846e-02,  6.04206184e-03,\n",
            "         8.74356250e-04, -1.10150594e-02,  5.75389015e-03,\n",
            "         1.53840156e-02,  9.80876945e-03,  2.02583969e-02,\n",
            "         2.27632504e-02,  3.81937064e-03,  1.88817028e-02,\n",
            "        -5.91993611e-03,  1.44126248e-02, -6.02060743e-03,\n",
            "         1.54689699e-02, -2.22433805e-02,  2.22195126e-02,\n",
            "        -3.39764706e-03,  4.26739780e-03, -5.51626831e-03,\n",
            "        -1.01848152e-02, -1.41909821e-02, -1.82390977e-02,\n",
            "        -1.61297061e-02, -2.73281103e-03, -1.64815958e-03,\n",
            "         6.31022034e-03, -1.61239076e-02, -1.79830045e-02,\n",
            "         1.38984935e-03,  1.74883809e-02, -2.21263878e-02,\n",
            "        -3.40057304e-03, -7.71544967e-03,  8.15085229e-03,\n",
            "        -6.00356935e-03, -9.10052285e-03,  1.77037083e-02,\n",
            "        -4.54338081e-02, -2.13592095e-04,  1.28148496e-02,\n",
            "        -1.71349826e-03,  2.81387911e-04,  5.99728152e-03,\n",
            "         1.63626261e-02,  1.71740428e-02,  1.11030936e-02,\n",
            "         1.26025230e-02,  8.71943682e-03, -1.02038160e-02,\n",
            "         8.40976555e-03,  3.43257859e-02,  1.13116752e-03,\n",
            "         3.17969173e-02, -1.06410114e-02, -1.06288074e-02,\n",
            "        -1.03880139e-02,  4.97767702e-03, -2.39774156e-02,\n",
            "         3.68182943e-03,  4.92782472e-03, -4.80936095e-03,\n",
            "         4.13990300e-03,  5.27110882e-03, -8.42626113e-03,\n",
            "         2.13964302e-02, -1.57073196e-02,  5.66763384e-03,\n",
            "        -3.50412726e-02, -2.06885897e-02, -2.63509974e-02,\n",
            "        -1.72006972e-02,  5.48695540e-03,  2.73980908e-02,\n",
            "         6.00695563e-03, -7.28768203e-03, -8.66592769e-03,\n",
            "        -8.26276094e-03, -2.19197180e-02,  1.20636234e-02,\n",
            "        -2.62593590e-02, -8.89830198e-03, -1.44515857e-02,\n",
            "         2.42903754e-02,  5.96991122e-05,  8.44748411e-03,\n",
            "        -1.35427061e-02, -2.42266227e-02,  3.09437737e-02,\n",
            "        -2.01305430e-02,  2.62076762e-02,  2.23129261e-02,\n",
            "         2.86373496e-02,  3.68019082e-02, -2.22148392e-02,\n",
            "         1.61076505e-02,  1.56056453e-02, -3.30821611e-03,\n",
            "        -3.08862049e-03,  8.72880104e-04,  2.81711295e-02,\n",
            "         1.70233697e-02, -2.64870487e-02,  9.49873496e-03,\n",
            "         4.78670653e-03, -1.50229651e-02, -2.70668622e-02,\n",
            "        -2.16376260e-02,  5.91651071e-03,  5.27595216e-03,\n",
            "        -6.56002993e-03, -1.76150203e-02,  2.17003711e-02,\n",
            "        -1.30503923e-02, -4.96873574e-04,  1.80918891e-02,\n",
            "        -9.00377426e-03,  4.12343256e-02,  9.48933698e-03,\n",
            "        -1.46100146e-03, -1.33357197e-02, -5.86130144e-03,\n",
            "        -3.43187153e-03, -4.51435335e-03,  1.18255382e-02,\n",
            "         3.74056166e-03,  2.24072021e-03, -1.98288653e-02,\n",
            "        -1.78745426e-02, -1.77439973e-02, -1.47817917e-02,\n",
            "        -2.28603613e-02, -5.85868582e-03, -5.91284828e-03,\n",
            "         1.24008153e-02,  1.67332161e-02]], dtype=float32)>))\n",
            "hi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-43452bd71ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-105-43452bd71ca4>\u001b[0m in \u001b[0;36mtext_generator\u001b[0;34m(_model, start, temperature)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msqueeze_v2\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m   4309\u001b[0m   \"\"\"\n\u001b[1;32m   4310\u001b[0m   \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(input, axis, name, squeeze_dims)\u001b[0m\n\u001b[1;32m   4257\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4258\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4259\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(input, axis, name)\u001b[0m\n\u001b[1;32m  10030\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10031\u001b[0m       return squeeze_eager_fallback(\n\u001b[0;32m> 10032\u001b[0;31m           input, axis=axis, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m  10033\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10034\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36msqueeze_eager_fallback\u001b[0;34m(input, axis, name, ctx)\u001b[0m\n\u001b[1;32m  10064\u001b[0m         \"'squeeze' Op, not %r.\" % axis)\n\u001b[1;32m  10065\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10066\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10067\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10068\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"squeeze_dims\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, default_dtype)\u001b[0m\n\u001b[1;32m    261\u001b[0m       ret.append(\n\u001b[1;32m    262\u001b[0m           ops.convert_to_tensor(\n\u001b[0;32m--> 263\u001b[0;31m               t, dtype, preferred_dtype=default_dtype, ctx=ctx))\n\u001b[0m\u001b[1;32m    264\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1500\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1436\u001b[0m           elems_as_tensors.append(\n\u001b[1;32m   1437\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0;32m-> 1438\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_elems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6457\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6458\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6459\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6461\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [1,2,12901] != values[1].shape = [2,1,200] [Op:Pack]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cj60MlKrNze"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6NNEVh2p4r9"
      },
      "source": [
        ""
      ]
    }
  ]
}